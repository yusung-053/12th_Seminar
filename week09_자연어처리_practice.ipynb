{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusung-053/12th_Seminar/blob/week09/week09_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자연어 처리 주요 코드\n",
        "처음 보는 코드들이라면 당연히 맞출 수 없습니다. 핵심적인 부분을 빈칸으로 만들었기 때문에 답지 보면서 확인하는 정도만 해보세요. 더 관심 있으시면 따로 더 찾아보시는걸 추천드립니다."
      ],
      "metadata": {
        "id": "RYsIUrF6Gd3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필수 라이브러리 설치\n",
        "!pip install scikit-learn\n",
        "!pip install gensim\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYTnQw97LHAi",
        "outputId": "fff556a0-dc72-4277-fb95-c235b5dc9e20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EEl6DPtCGGo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6523a552-8c5f-43fb-e585-f194a0714126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 어휘 사전 (Vocabulary) ---\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "--- BoW 행렬 (결과) ---\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "# [퀴즈 1: BoW (CountVectorizer)]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # 1번 빈칸 (BoW 모델)\n",
        "\n",
        "# 2. 분석할 문서 데이터\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# 3. BoW Vectorizer 객체 생성\n",
        "# (내부적으로 토큰화, 불용어 제거 등을 수행합니다)\n",
        "vectorizer = CountVectorizer()  # 1번 빈칸 (위와 동일)\n",
        "\n",
        "# 4. 문서를 BoW 행렬로 학습 및 변환\n",
        "bow_matrix = vectorizer.fit_transform(corpus)  # 2번 빈칸 (학습 및 변환)\n",
        "\n",
        "print(\"--- 어휘 사전 (Vocabulary) ---\")\n",
        "# 'get_feature_names_out'은 학습된 단어 목록(사전)을 보여줍니다.\n",
        "print(vectorizer.get_feature_names_out())  # 3번 빈칸 (학습된 단어 목록 가져오기)\n",
        "\n",
        "print(\"--- BoW 행렬 (결과) ---\")\n",
        "print(bow_matrix.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [퀴즈 2: Word2Vec (gensim)]\n",
        "\n",
        "from gensim.models import Word2Vec  # 1번 빈칸 (Word2Vec 모델)\n",
        "\n",
        "# 2. 학습할 문장 데이터 (이미 토큰화 완료)\n",
        "sentences = [\n",
        "    ['I', 'love', 'nlp'],\n",
        "    ['I', 'love', 'deep', 'learning'],\n",
        "    ['nlp', 'is', 'fun'],\n",
        "    ['deep', 'learning', 'is', 'fun']\n",
        "]\n",
        "\n",
        "# 3. Word2Vec 모델 학습\n",
        "# vector_size=100 : 단어를 100차원 벡터로 만듦\n",
        "# window=2 : 주변 2개 단어까지 문맥으로 참고\n",
        "# min_count=1 : 최소 1번 이상 나온 단어만 학습\n",
        "model = Word2Vec(\n",
        "    sentences=sentences,  # 2번 빈칸 (학습 데이터)\n",
        "    vector_size=100,\n",
        "    window=2,\n",
        "    min_count=1\n",
        ")\n",
        "\n",
        "# 4. 'nlp' 단어의 학습된 벡터 확인\n",
        "vector_nlp = model.wv['nlp']  # 3번 빈칸 (단어 벡터 접근)\n",
        "\n",
        "print(f\"--- 'nlp'의 100차원 벡터 ---\")\n",
        "print(vector_nlp)\n",
        "\n",
        "# 5. 'nlp'와 가장 유사한 단어 찾기\n",
        "similar_words = model.wv.most_similar('nlp')  # 4번 빈칸 (유사 단어 찾기)\n",
        "print(f\"\\n--- 'nlp'와 가장 유사한 단어 ---\")\n",
        "print(similar_words)"
      ],
      "metadata": {
        "id": "I65olYhOGQHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca6e580-ef66-471c-ecb6-569bf7bb76a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 'nlp'의 100차원 벡터 ---\n",
            "[-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
            "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
            " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
            "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
            " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
            "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
            " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
            " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
            "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
            " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
            "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
            "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
            " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
            "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
            " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
            "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
            "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
            "\n",
            "--- 'nlp'와 가장 유사한 단어 ---\n",
            "[('learning', 0.17018885910511017), ('love', 0.13887983560562134), ('I', 0.03476494923233986), ('is', 0.004503022879362106), ('fun', -0.027750348672270775), ('deep', -0.04461711645126343)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential     # 1번 빈칸 (모델 뼈대)\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense  # 2, 3, 4번 빈칸\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer      # 5번 빈칸 (전처리)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # 6번 빈칸 (전처리)\n",
        "\n",
        "# 1. 데이터 준비 (긍정=1, 부정=0)\n",
        "texts = ['This movie is great', 'I love this film', 'This is terrible', 'I hate this movie']\n",
        "labels = np.array([1, 1, 0, 0])\n",
        "\n",
        "# 2. 텍스트 전처리 (Tokenizer)\n",
        "vocab_size = 100  # 단어 사전 크기를 100개로 제한\n",
        "tokenizer = Tokenizer(num_words=vocab_size)  # 5번 빈칸\n",
        "tokenizer.fit_on_texts(texts)            # 7번 빈칸 (단어 사전 학습)\n",
        "sequences = tokenizer.texts_to_sequences(texts)       # 8번 빈칸 (텍스트를 숫자 시퀀스로)\n",
        "\n",
        "print(\"--- 숫자 시퀀스 변환 ---\")\n",
        "print(sequences)\n",
        "\n",
        "# 3. 텍스트 전처리 (Padding)\n",
        "max_len = 10  # 모든 문장 길이를 10으로 맞춤\n",
        "padded_data = pad_sequences(sequences, maxlen=max_len)  # 6번 빈칸\n",
        "\n",
        "print(\"\\n--- 패딩 완료 (길이 10) ---\")\n",
        "print(padded_data)\n",
        "\n",
        "# 4. 모델 구축\n",
        "model = Sequential()  # 1번 빈칸\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_len)) # 2번 빈칸\n",
        "model.add(LSTM(units=64))  # 3번 빈칸\n",
        "model.add(Dense(units=1, activation='sigmoid'))  # 4번 빈칸\n",
        "\n",
        "# 5. 모델 컴파일 (학습 설정)\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',  # 9번, 10번 빈칸\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# 6. 모델 훈련\n",
        "model.fit(padded_data, labels, epochs=20)  # 11번, 12번 빈칸\n",
        "\n",
        "# 7. [퀴즈] 새로운 데이터 예측\n",
        "test_text = ['I love this great movie']\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len)\n",
        "\n",
        "prediction = model.predict(test_pad)  # 13번 빈칸\n",
        "print(f\"\\n--- 예측 결과 ('I love this great movie'는?) ---\")\n",
        "print(prediction) # 1에 가까우면 긍정"
      ],
      "metadata": {
        "id": "cpsjl3rVGT-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a7f179-1668-4ca3-987c-8d5da49c0f16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 숫자 시퀀스 변환 ---\n",
            "[[1, 2, 3, 5], [4, 6, 1, 7], [1, 3, 8], [4, 9, 1, 2]]\n",
            "\n",
            "--- 패딩 완료 (길이 10) ---\n",
            "[[0 0 0 0 0 0 1 2 3 5]\n",
            " [0 0 0 0 0 0 4 6 1 7]\n",
            " [0 0 0 0 0 0 0 1 3 8]\n",
            " [0 0 0 0 0 0 4 9 1 2]]\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6919\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6908\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 0.6897\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7500 - loss: 0.6886\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6875\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.6863\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6850\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6837\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6824\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6809\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.6793\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6777\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6759\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6740\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.6719\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.6696\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6672\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6646\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.6617\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6586\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "\n",
            "--- 예측 결과 ('I love this great movie'는?) ---\n",
            "[[0.51201576]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어 처리에서 가장 유명한 논문이 \"Attention Is All You Need\"입니다. 자연어 처리에 관심이 있으신 분들인 이 논문을 꼭 읽어보시는걸 추천드립니다. (읽으면서 모르는건 생성형 AI에 질문해보세요.)"
      ],
      "metadata": {
        "id": "NAlJH6fJHXmX"
      }
    }
  ]
}